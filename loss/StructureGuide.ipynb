{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:16:31.233880Z",
     "start_time": "2023-12-10T17:16:31.229752Z"
    }
   },
   "outputs": [],
   "source": [
    "# So we will combine the ideas from the losses we have experimented with \n",
    "# InverseDepth Loss works fine if there is no feathering in the subject . But if we penalize also on incorrectly detecting edges . we might improve upon the loss . So a edge-guide is needed\n",
    "\n",
    "# And we will linearly combine with the loss that we experimented with Online sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from data.loaders.DataLoader import RedWebDataset , Rescale , RandomCrop\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "normal_dataset = RedWebDataset(root_dir=\"../data/ReDWeb_V1\",transform=transforms.Compose([\n",
    "    Rescale((256,256)),\n",
    "]))\n",
    "batcher = DataLoader(normal_dataset,batch_size=1,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:21:34.660344Z",
     "start_time": "2023-12-10T17:21:34.650868Z"
    }
   },
   "id": "9e4c22576c6ea426"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Lets Device our online sampling first\n",
    "\n",
    "# Note this is derived completely from the Redweb paper\n",
    "def onlineSampling(inputs, targets, masks, threshold, sample_num):\n",
    "\n",
    "    # find A-B point pairs from predictions (mostly random)\n",
    "    inputs_index = torch.masked_select(inputs, targets.gt(threshold))\n",
    "    num_effect_pixels = len(inputs_index)\n",
    "    shuffle_effect_pixels = torch.randperm(num_effect_pixels).cuda()\n",
    "    rgb_a = inputs_index[shuffle_effect_pixels[0:sample_num*2:2]]\n",
    "    rgb_b = inputs_index[shuffle_effect_pixels[1:sample_num*2:2]]\n",
    "\n",
    "    # find corresponding pairs from ground truth\n",
    "    depth_index = torch.masked_select(targets, targets.gt(threshold))\n",
    "    depth_a = depth_index[shuffle_effect_pixels[0:sample_num*2:2]]\n",
    "    depth_b = depth_index[shuffle_effect_pixels[1:sample_num*2:2]]\n",
    "\n",
    "    # only compute the losses of point pairs with valid ground truth i.e consistent masked\n",
    "    consistent_masks_index = torch.masked_select(masks, targets.gt(threshold))\n",
    "    consistent_masks_A = consistent_masks_index[shuffle_effect_pixels[0:sample_num*2:2]]\n",
    "    consistent_masks_B = consistent_masks_index[shuffle_effect_pixels[1:sample_num*2:2]]\n",
    "\n",
    "    # The amount of A and B should be the same!!\n",
    "    if len(depth_a) > len(depth_b):\n",
    "        depth_a = depth_a[:-1]\n",
    "        rgb_a = rgb_a[:-1]\n",
    "        consistent_masks_A = consistent_masks_A[:-1]\n",
    "\n",
    "    return rgb_a, rgb_b, depth_a, depth_b, consistent_masks_A, consistent_masks_B\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:32:20.008755Z",
     "start_time": "2023-12-10T17:32:20.006455Z"
    }
   },
   "id": "c64b7b829e8a27fb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# now lets penalize wrong edges\n",
    "# i.e if the edeges derived from the depth map does not map with the edges derived from the original image. there should be a corresponding penalty foir it.\n",
    "\n",
    "# convenience wrapper function to get pixels\n",
    "def ind2sub(idx, cols):\n",
    "    r = idx / cols\n",
    "    c = idx - r * cols\n",
    "    return r, c\n",
    "\n",
    "\n",
    "def sub2ind(r, c, cols):\n",
    "    idx = r * cols + c\n",
    "    return idx\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T18:26:27.067172Z",
     "start_time": "2023-12-10T18:26:27.061371Z"
    }
   },
   "id": "b2e6d82bd81241fd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def edgeGuidedSampling(inputs, targets, edges_img, thetas_img, masks, h, w):\n",
    "\n",
    "    # find edges\n",
    "    edges_max = edges_img.max()\n",
    "    edges_mask = edges_img.ge(edges_max*0.1)\n",
    "    edges_loc = edges_mask.nonzero()\n",
    "\n",
    "    inputs_edge = torch.masked_select(inputs, edges_mask)\n",
    "    targets_edge = torch.masked_select(targets, edges_mask)\n",
    "    thetas_edge = torch.masked_select(thetas_img, edges_mask)\n",
    "    minlen = inputs_edge.size()[0]\n",
    "\n",
    "    # find anchor points (i.e, edge points)\n",
    "    sample_num = minlen\n",
    "    index_anchors = torch.randint(\n",
    "        0, minlen, (sample_num,), dtype=torch.long).cuda()\n",
    "    anchors = torch.gather(inputs_edge, 0, index_anchors)\n",
    "    theta_anchors = torch.gather(thetas_edge, 0, index_anchors)\n",
    "    row_anchors, col_anchors = ind2sub(edges_loc[index_anchors].squeeze(1), w)\n",
    "    # compute the coordinates of 4-points,  distances are from [2, 30]\n",
    "    distance_matrix = torch.randint(2, 31, (4, sample_num)).cuda()\n",
    "    pos_or_neg = torch.ones(4, sample_num).cuda()\n",
    "    pos_or_neg[:2, :] = -pos_or_neg[:2, :]\n",
    "    distance_matrix = distance_matrix.float() * pos_or_neg\n",
    "    col = col_anchors.unsqueeze(0).expand(4, sample_num).long(\n",
    "    ) + torch.round(distance_matrix.double() * torch.cos(theta_anchors).unsqueeze(0)).long()\n",
    "    row = row_anchors.unsqueeze(0).expand(4, sample_num).long(\n",
    "    ) + torch.round(distance_matrix.double() * torch.sin(theta_anchors).unsqueeze(0)).long()\n",
    "\n",
    "    # constrain 0=<c<=w, 0<=r<=h\n",
    "    # Note: index should minus 1\n",
    "    col[col < 0] = 0\n",
    "    col[col > w-1] = w-1\n",
    "    row[row < 0] = 0\n",
    "    row[row > h-1] = h-1\n",
    "\n",
    "    # a-b, b-c, c-d\n",
    "    a = sub2ind(row[0, :], col[0, :], w)\n",
    "    b = sub2ind(row[1, :], col[1, :], w)\n",
    "    c = sub2ind(row[2, :], col[2, :], w)\n",
    "    d = sub2ind(row[3, :], col[3, :], w)\n",
    "    A = torch.cat((a, b, c), 0)\n",
    "    B = torch.cat((b, c, d), 0)\n",
    "\n",
    "    rgb_a = torch.gather(inputs, 0, A.long())\n",
    "    rgb_b = torch.gather(inputs, 0, B.long())\n",
    "    depth_a = torch.gather(targets, 0, A.long())\n",
    "    depth_b = torch.gather(targets, 0, B.long())\n",
    "    masks_A = torch.gather(masks, 0, A.long())\n",
    "    masks_B = torch.gather(masks, 0, B.long())\n",
    "\n",
    "    return rgb_a, rgb_b, depth_a, depth_b, masks_A, masks_B, sample_num"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T18:26:39.781607Z",
     "start_time": "2023-12-10T18:26:39.768493Z"
    }
   },
   "id": "1e8498b09b496b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EdgeguidedRankingLoss(nn.Module):\n",
    "    def __init__(self, point_pairs=10000, sigma=0.03, alpha=1.0, mask_value=-1e-8):\n",
    "        super(EdgeguidedRankingLoss, self).__init__()\n",
    "        self.point_pairs = point_pairs  # number of point pairs\n",
    "        self.sigma = sigma  # used for determining the ordinal relationship between a selected pair\n",
    "        self.alpha = alpha  # used for balancing the effect of = and (<,>)\n",
    "        self.mask_value = mask_value\n",
    "        # self.regularization_loss = GradientLoss(scales=4)\n",
    "\n",
    "    def getEdge(self, images):\n",
    "        n, c, h, w = images.size()\n",
    "        a = torch.Tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n",
    "                         ).cuda().view((1, 1, 3, 3)).repeat(1, 1, 1, 1)\n",
    "        b = torch.Tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
    "                         ).cuda().view((1, 1, 3, 3)).repeat(1, 1, 1, 1)\n",
    "        if c == 3:\n",
    "            gradient_x = F.conv2d(images[:, 0, :, :].unsqueeze(1), a)\n",
    "            gradient_y = F.conv2d(images[:, 0, :, :].unsqueeze(1), b)\n",
    "        else:\n",
    "            gradient_x = F.conv2d(images, a)\n",
    "            gradient_y = F.conv2d(images, b)\n",
    "        edges = torch.sqrt(torch.pow(gradient_x, 2) + torch.pow(gradient_y, 2))\n",
    "        edges = F.pad(edges, (1, 1, 1, 1), \"constant\", 0)\n",
    "        thetas = torch.atan2(gradient_y, gradient_x)\n",
    "        thetas = F.pad(thetas, (1, 1, 1, 1), \"constant\", 0)\n",
    "\n",
    "        return edges, thetas\n",
    "\n",
    "    def forward(self, inputs, targets, images, masks=None):\n",
    "        if masks == None:\n",
    "            masks = targets > self.mask_value\n",
    "        # Comment this line if you don't want to use the multi-scale gradient matching term !!!\n",
    "        # regularization_loss = self.regularization_loss(inputs.squeeze(1), targets.squeeze(1), masks.squeeze(1))\n",
    "        # find edges from RGB\n",
    "        edges_img, thetas_img = self.getEdge(images)\n",
    "\n",
    "        # =============================\n",
    "        n, c, h, w = targets.size()\n",
    "        if n != 1:\n",
    "            inputs = inputs.view(n, -1).double()\n",
    "            targets = targets.view(n, -1).double()\n",
    "            masks = masks.view(n, -1).double()\n",
    "            edges_img = edges_img.view(n, -1).double()\n",
    "            thetas_img = thetas_img.view(n, -1).double()\n",
    "\n",
    "        else:\n",
    "            inputs = inputs.contiguous().view(1, -1).double()\n",
    "            targets = targets.contiguous().view(1, -1).double()\n",
    "            masks = masks.contiguous().view(1, -1).double()\n",
    "            edges_img = edges_img.contiguous().view(1, -1).double()\n",
    "            thetas_img = thetas_img.contiguous().view(1, -1).double()\n",
    "\n",
    "        # initialization\n",
    "        loss = torch.DoubleTensor([0.0]).cuda()\n",
    "\n",
    "        for i in range(n):\n",
    "            # Edge-Guided sampling\n",
    "            rgb_a, rgb_b, depth_a, depth_b, masks_A, masks_B, sample_num = edgeGuidedSampling(\n",
    "                inputs[i, :], targets[i, :], edges_img[i], thetas_img[i], masks[i, :], h, w)\n",
    "            # Random Sampling\n",
    "            random_sample_num = sample_num\n",
    "            random_rgb_a, random_rgb_b, random_depth_a, random_depth_b, random_masks_A, random_masks_B = onlineSampling(\n",
    "                inputs[i, :], targets[i, :], masks[i, :], self.mask_value, random_sample_num)\n",
    "\n",
    "            # Combine EGS + RS\n",
    "            rgb_a = torch.cat((rgb_a, random_rgb_a), 0)\n",
    "            rgb_b = torch.cat((rgb_b, random_rgb_b), 0)\n",
    "            depth_a = torch.cat((depth_a, random_depth_a), 0)\n",
    "            depth_b = torch.cat((depth_b, random_depth_b), 0)\n",
    "            masks_A = torch.cat((masks_A, random_masks_A), 0)\n",
    "            masks_B = torch.cat((masks_B, random_masks_B), 0)\n",
    "\n",
    "            # GT ordinal relationship\n",
    "            target_ratio = torch.div(depth_a+1e-6, depth_b+1e-6)\n",
    "            mask_eq = target_ratio.lt(\n",
    "                1.0 + self.sigma) * target_ratio.gt(1.0/(1.0+self.sigma))\n",
    "            labels = torch.zeros_like(target_ratio)\n",
    "            labels[target_ratio.ge(1.0 + self.sigma)] = 1\n",
    "            labels[target_ratio.le(1.0/(1.0+self.sigma))] = -1\n",
    "\n",
    "            # consider forward-backward consistency checking, i.e, only compute losses of point pairs with valid GT\n",
    "            consistency_mask = masks_A * masks_B\n",
    "\n",
    "            equal_loss = (rgb_a - rgb_b).pow(2) * \\\n",
    "                         mask_eq.double() * consistency_mask\n",
    "            unequal_loss = torch.log(\n",
    "                1 + torch.exp((-rgb_a + rgb_b) * labels)) * (~mask_eq).double() * consistency_mask\n",
    "\n",
    "            # Please comment the regularization term if you don't want to use the multi-scale gradient matching loss !!!\n",
    "            # + 0.2 * regularization_loss.double()\n",
    "            loss = loss + self.alpha * equal_loss.mean() + 1.0 * unequal_loss.mean()\n",
    "\n",
    "        return loss[0].float()/n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81f78d46eab54cd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
